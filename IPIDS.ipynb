{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here are some imports that are used along this notebook\n",
    "import math\n",
    "import itertools\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "gt0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext, Row\n",
    "sc = pyspark.SparkContext(master='local[8]')\n",
    "sc.setLogLevel('INFO')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, split, col\n",
    "import pyspark.sql.functions as sql\n",
    "\n",
    "train20_nsl_kdd_dataset_path = \"NSL_KDD_Dataset/KDDTrain+_20Percent.txt\"\n",
    "train_nsl_kdd_dataset_path = \"NSL_KDD_Dataset/KDDTrain+.txt\"\n",
    "test_nsl_kdd_dataset_path = \"NSL_KDD_Dataset/KDDTest+.txt\"\n",
    "\n",
    "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"])\n",
    "\n",
    "nominal_inx = [1, 2, 3]\n",
    "binary_inx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_inx = list(set(range(41)).difference(nominal_inx).difference(binary_inx))\n",
    "\n",
    "nominal_cols = col_names[nominal_inx].tolist()\n",
    "binary_cols = col_names[binary_inx].tolist()\n",
    "numeric_cols = col_names[numeric_inx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to load dataset and divide it into 8 partitions\n",
    "def load_dataset(path):\n",
    "    dataset_rdd = sc.textFile(path, 8).map(lambda line: line.split(','))\n",
    "    dataset_df = (dataset_rdd.toDF(col_names.tolist()).select(\n",
    "                    col('duration').cast(DoubleType()),\n",
    "                    col('protocol_type').cast(StringType()),\n",
    "                    col('service').cast(StringType()),\n",
    "                    col('flag').cast(StringType()),\n",
    "                    col('src_bytes').cast(DoubleType()),\n",
    "                    col('dst_bytes').cast(DoubleType()),\n",
    "                    col('land').cast(DoubleType()),\n",
    "                    col('wrong_fragment').cast(DoubleType()),\n",
    "                    col('urgent').cast(DoubleType()),\n",
    "                    col('hot').cast(DoubleType()),\n",
    "                    col('num_failed_logins').cast(DoubleType()),\n",
    "                    col('logged_in').cast(DoubleType()),\n",
    "                    col('num_compromised').cast(DoubleType()),\n",
    "                    col('root_shell').cast(DoubleType()),\n",
    "                    col('su_attempted').cast(DoubleType()),\n",
    "                    col('num_root').cast(DoubleType()),\n",
    "                    col('num_file_creations').cast(DoubleType()),\n",
    "                    col('num_shells').cast(DoubleType()),\n",
    "                    col('num_access_files').cast(DoubleType()),\n",
    "                    col('num_outbound_cmds').cast(DoubleType()),\n",
    "                    col('is_host_login').cast(DoubleType()),\n",
    "                    col('is_guest_login').cast(DoubleType()),\n",
    "                    col('count').cast(DoubleType()),\n",
    "                    col('srv_count').cast(DoubleType()),\n",
    "                    col('serror_rate').cast(DoubleType()),\n",
    "                    col('srv_serror_rate').cast(DoubleType()),\n",
    "                    col('rerror_rate').cast(DoubleType()),\n",
    "                    col('srv_rerror_rate').cast(DoubleType()),\n",
    "                    col('same_srv_rate').cast(DoubleType()),\n",
    "                    col('diff_srv_rate').cast(DoubleType()),\n",
    "                    col('srv_diff_host_rate').cast(DoubleType()),\n",
    "                    col('dst_host_count').cast(DoubleType()),\n",
    "                    col('dst_host_srv_count').cast(DoubleType()),\n",
    "                    col('dst_host_same_srv_rate').cast(DoubleType()),\n",
    "                    col('dst_host_diff_srv_rate').cast(DoubleType()),\n",
    "                    col('dst_host_same_src_port_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_diff_host_rate').cast(DoubleType()),\n",
    "                    col('dst_host_serror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_serror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_rerror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_rerror_rate').cast(DoubleType()),\n",
    "                    col('labels').cast(StringType())))\n",
    "\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "# Dictionary that contains mapping of various attacks to the four main categories\n",
    "attack_dict = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "attack_mapping_udf = udf(lambda v: attack_dict[v])\n",
    "\n",
    "class Labels2Converter(Transformer):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(Labels2Converter, self).__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn('labels2', sql.regexp_replace(col('labels'), '^(?!normal).*$', 'attack'))\n",
    "     \n",
    "class Labels5Converter(Transformer):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(Labels5Converter, self).__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn('labels5', attack_mapping_udf(col('labels')))\n",
    "    \n",
    "labels2_indexer = StringIndexer(inputCol=\"labels2\", outputCol=\"labels2_index\")\n",
    "labels5_indexer = StringIndexer(inputCol=\"labels5\", outputCol=\"labels5_index\")\n",
    "\n",
    "labels_mapping_pipeline = Pipeline(stages=[Labels2Converter(), Labels5Converter(), labels2_indexer, labels5_indexer])\n",
    "\n",
    "labels2 = ['normal', 'attack']\n",
    "labels5 = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
    "labels_col = 'labels2_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "26.95871615409851\n"
     ]
    }
   ],
   "source": [
    "# Loading train data\n",
    "t0 = time()\n",
    "train_df = load_dataset(train_nsl_kdd_dataset_path)\n",
    "\n",
    "# Fitting preparation pipeline\n",
    "labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "2.9956207275390625\n"
     ]
    }
   ],
   "source": [
    "# Loading test data\n",
    "t0 = time()\n",
    "test_df = load_dataset(test_nsl_kdd_dataset_path)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ohe_vec(cat_dict, row):\n",
    "    vec = np.zeros(len(cat_dict))\n",
    "    vec[cat_dict[row]] = float(1.0)\n",
    "    return vec.tolist()\n",
    "\n",
    "def ohe(df, nominal_col):\n",
    "    categories = (df.select(nominal_col)\n",
    "                    .distinct()\n",
    "                    .rdd.map(lambda row: row[0])\n",
    "                    .collect())\n",
    "    \n",
    "    cat_dict = dict(zip(categories, range(len(categories))))\n",
    "    \n",
    "    udf_ohe_vec = udf(lambda row: ohe_vec(cat_dict, row), \n",
    "                      StructType([StructField(cat, DoubleType(), False) for cat in categories]))\n",
    "    \n",
    "    df = df.withColumn(nominal_col + '_ohe', udf_ohe_vec(col(nominal_col))).cache()\n",
    "    \n",
    "    nested_cols = [nominal_col + '_ohe.' + cat for cat in categories]\n",
    "    ohe_cols = [nominal_col + '_' + cat for cat in categories]\n",
    "        \n",
    "    for new, old in zip(ohe_cols, nested_cols):\n",
    "        df = df.withColumn(new, col(old))\n",
    "\n",
    "    df = df.drop(nominal_col + '_ohe')\n",
    "                   \n",
    "    return df, ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "42.5325984954834\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "train_ohe_cols = []\n",
    "\n",
    "train_df, train_ohe_col0 = ohe(train_df, nominal_cols[0])\n",
    "train_ohe_cols += train_ohe_col0\n",
    "\n",
    "train_df, train_ohe_col1 = ohe(train_df, nominal_cols[1])\n",
    "train_ohe_cols += train_ohe_col1\n",
    "\n",
    "train_df, train_ohe_col2 = ohe(train_df, nominal_cols[2])\n",
    "train_ohe_cols += train_ohe_col2\n",
    "\n",
    "binary_cols += train_ohe_cols\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "17.54893660545349\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "test_ohe_cols = []\n",
    "\n",
    "test_df, test_ohe_col0_names = ohe(test_df, nominal_cols[0])\n",
    "test_ohe_cols += test_ohe_col0_names\n",
    "\n",
    "test_df, test_ohe_col1_names = ohe(test_df, nominal_cols[1])\n",
    "test_ohe_cols += test_ohe_col1_names\n",
    "\n",
    "test_df, test_ohe_col2_names = ohe(test_df, nominal_cols[2])\n",
    "test_ohe_cols += test_ohe_col2_names\n",
    "\n",
    "test_binary_cols = col_names[binary_inx].tolist() + test_ohe_cols\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAttributeRatio(df, numericCols, binaryCols, labelCol):\n",
    "    ratio_dict = {}\n",
    "    \n",
    "    if numericCols:\n",
    "        avg_dict = (df\n",
    "                .select(list(map(lambda c: sql.avg(c).alias(c), numericCols)))\n",
    "                .first()\n",
    "                .asDict())\n",
    "\n",
    "        ratio_dict.update(df\n",
    "                .groupBy(labelCol)\n",
    "                .avg(*numericCols)\n",
    "                .select(list(map(lambda c: sql.max(col('avg(' + c + ')')/avg_dict[c]).alias(c), numericCols)))\n",
    "                .fillna(0.0)\n",
    "                .first()\n",
    "                .asDict())\n",
    "    \n",
    "    if binaryCols:\n",
    "        ratio_dict.update((df\n",
    "                .groupBy(labelCol)\n",
    "                .agg(*list(map(lambda c: (sql.sum(col(c))/(sql.count(col(c)) - sql.sum(col(c)))).alias(c), binaryCols)))\n",
    "                .fillna(1000.0)\n",
    "                .select(*list(map(lambda c: sql.max(col(c)).alias(c), binaryCols)))\n",
    "                .first()\n",
    "                .asDict()))\n",
    "        \n",
    "    return OrderedDict(sorted(ratio_dict.items(), key=lambda v: -v[1]))\n",
    "\n",
    "def selectFeaturesByAR(ar_dict, min_ar):\n",
    "    return [f for f in ar_dict.keys() if ar_dict[f] >= min_ar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "14.225703954696655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('protocol_type_tcp', 1000.0),\n",
       "             ('num_shells', 326.11353550295854),\n",
       "             ('urgent', 173.03983516483518),\n",
       "             ('num_file_creations', 62.23362492770388),\n",
       "             ('flag_SF', 51.0),\n",
       "             ('num_failed_logins', 46.03855641845592),\n",
       "             ('hot', 40.77451681709518),\n",
       "             ('logged_in', 10.569767441860465),\n",
       "             ('dst_bytes', 9.154854355343401),\n",
       "             ('src_bytes', 8.464064204948945),\n",
       "             ('duration', 7.225829157212557),\n",
       "             ('dst_host_srv_diff_host_rate', 5.756880682756574),\n",
       "             ('dst_host_diff_srv_rate', 4.83734184897426),\n",
       "             ('num_access_files', 4.694879248658319),\n",
       "             ('dst_host_same_src_port_rate', 4.393080378884017),\n",
       "             ('num_compromised', 4.338539274983927),\n",
       "             ('diff_srv_rate', 4.069085485070395),\n",
       "             ('dst_host_srv_rerror_rate', 3.667920527965924),\n",
       "             ('srv_rerror_rate', 3.667741802325429),\n",
       "             ('rerror_rate', 3.645586087828447),\n",
       "             ('dst_host_rerror_rate', 3.2795669242444494),\n",
       "             ('srv_diff_host_rate', 3.0815657101103984),\n",
       "             ('flag_S0', 2.965034965034965),\n",
       "             ('wrong_fragment', 2.742896335488928),\n",
       "             ('dst_host_srv_serror_rate', 2.6731595957140732),\n",
       "             ('srv_serror_rate', 2.643246318490161),\n",
       "             ('serror_rate', 2.6310546426370265),\n",
       "             ('dst_host_serror_rate', 2.6293396511768043),\n",
       "             ('num_root', 2.6091432537726016),\n",
       "             ('count', 2.1174082949142403),\n",
       "             ('service_telnet', 1.8888888888888888),\n",
       "             ('dst_host_srv_count', 1.6453161847397422),\n",
       "             ('dst_host_same_srv_rate', 1.557578827974319),\n",
       "             ('service_ftp_data', 1.5447570332480818),\n",
       "             ('same_srv_rate', 1.5079612006047083),\n",
       "             ('dst_host_count', 1.3428596865228266),\n",
       "             ('service_http', 1.2988666621151088),\n",
       "             ('srv_count', 1.1773191099992069),\n",
       "             ('root_shell', 1.0),\n",
       "             ('service_private', 0.7252812314979278),\n",
       "             ('protocol_type_icmp', 0.5497939103842574),\n",
       "             ('service_eco_i', 0.5403726708074534),\n",
       "             ('is_guest_login', 0.45894428152492667),\n",
       "             ('service_ftp', 0.4568081991215227),\n",
       "             ('flag_REJ', 0.3265050642995334),\n",
       "             ('flag_RSTR', 0.23005487547488393),\n",
       "             ('protocol_type_udp', 0.22644739478045495),\n",
       "             ('service_other', 0.16945921541085582),\n",
       "             ('service_domain_u', 0.15493320070658045),\n",
       "             ('service_smtp', 0.11654010677454654),\n",
       "             ('service_ecr_i', 0.06601211614790056),\n",
       "             ('flag_RSTO', 0.04847207586933614),\n",
       "             ('service_finger', 0.026095310440358364),\n",
       "             ('flag_SH', 0.02326398033535247),\n",
       "             ('service_Z39_50', 0.018879226195758277),\n",
       "             ('service_uucp', 0.01702909783427078),\n",
       "             ('service_courier', 0.0160615915577089),\n",
       "             ('service_auth', 0.015544843445957898),\n",
       "             ('service_bgp', 0.0154550278588485),\n",
       "             ('service_uucp_path', 0.014938896377980597),\n",
       "             ('service_iso_tsap', 0.014916467780429593),\n",
       "             ('service_whois', 0.014804339660163068),\n",
       "             ('service_nnsp', 0.013729168965897804),\n",
       "             ('service_imap4', 0.013729168965897804),\n",
       "             ('service_vmnet', 0.013371284834844774),\n",
       "             ('service_time', 0.012142983074753174),\n",
       "             ('service_ctf', 0.011853092158893123),\n",
       "             ('service_csnet_ns', 0.011741639864299247),\n",
       "             ('service_supdup', 0.011630212119209674),\n",
       "             ('service_http_443', 0.011518808915514052),\n",
       "             ('service_discard', 0.011451978769793203),\n",
       "             ('service_domain', 0.011184746471740902),\n",
       "             ('service_daytime', 0.01107344135258894),\n",
       "             ('service_gopher', 0.010672945733022314),\n",
       "             ('service_efs', 0.010517283108539242),\n",
       "             ('service_exec', 0.010228322555100963),\n",
       "             ('service_systat', 0.010117227879561),\n",
       "             ('service_link', 0.009983946517713808),\n",
       "             ('service_hostnames', 0.00982849604221636),\n",
       "             ('service_name', 0.009406800149453835),\n",
       "             ('service_klogin', 0.009340248780273395),\n",
       "             ('service_login', 0.009229349330872173),\n",
       "             ('service_mtp', 0.009140647316033486),\n",
       "             ('service_echo', 0.009140647316033486),\n",
       "             ('service_urp_i', 0.0089745894762076),\n",
       "             ('flag_RSTOS0', 0.008915433220808448),\n",
       "             ('service_ldap', 0.008852473420613302),\n",
       "             ('service_netbios_dgm', 0.008608762490392005),\n",
       "             ('service_sunrpc', 0.00809956538917424),\n",
       "             ('service_netbios_ssn', 0.007657203036552723),\n",
       "             ('service_netstat', 0.0075466731018142726),\n",
       "             ('service_netbios_ns', 0.007369875633348687),\n",
       "             ('service_kshell', 0.006398597567656404),\n",
       "             ('service_nntp', 0.006156070630504316),\n",
       "             ('service_ssh', 0.006156070630504316),\n",
       "             ('flag_S1', 0.005389507628915231),\n",
       "             ('service_sql_net', 0.005099137742373178),\n",
       "             ('flag_S3', 0.0030241935483870967),\n",
       "             ('flag_OTH', 0.0030117890026675844),\n",
       "             ('service_pop_3', 0.0027696293759399615),\n",
       "             ('service_IRC', 0.0027696293759399615),\n",
       "             ('service_ntp_u', 0.0025009304056568663),\n",
       "             ('su_attempted', 0.0020534186444460976),\n",
       "             ('flag_S2', 0.0017702011186481019),\n",
       "             ('service_remote_job', 0.0015466575012888812),\n",
       "             ('service_rje', 0.0015466575012888812),\n",
       "             ('service_pop_2', 0.0015264845061822622),\n",
       "             ('service_printer', 0.0013517933064428214),\n",
       "             ('service_shell', 0.0011553385359898854),\n",
       "             ('service_X11', 0.0009958974968785302),\n",
       "             ('service_pm_dump', 0.0004291477126426916),\n",
       "             ('land', 0.00039207998431680063),\n",
       "             ('service_harvest', 0.00017161489617298782),\n",
       "             ('service_aol', 0.00017161489617298782),\n",
       "             ('service_http_8001', 0.00017161489617298782),\n",
       "             ('service_urh_i', 0.0001485155867108253),\n",
       "             ('service_red_i', 0.00011880894037276305),\n",
       "             ('service_http_2784', 8.58000858000858e-05),\n",
       "             ('service_tim_i', 7.425227954498203e-05),\n",
       "             ('service_tftp_u', 4.455004455004455e-05),\n",
       "             ('is_host_login', 1.4849573817231445e-05),\n",
       "             ('num_outbound_cmds', 0.0)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time()\n",
    "ar_dict = getAttributeRatio(train_df, numeric_cols, binary_cols, 'labels5')\n",
    "\n",
    "print(len(ar_dict))\n",
    "print(time() - t0)\n",
    "ar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "22544\n",
      "8.52225136756897\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "avg_dict = (train_df.select(list(map(lambda c: sql.avg(c).alias(c), numeric_cols))).first().asDict())\n",
    "std_dict = (train_df.select(list(map(lambda c: sql.stddev(c).alias(c), numeric_cols))).first().asDict())\n",
    "\n",
    "def standardizer(column):\n",
    "    return ((col(column) - avg_dict[column])/std_dict[column]).alias(column)\n",
    "\n",
    "train_scaler = [*binary_cols, *list(map(standardizer, numeric_cols)), *['id', 'labels2_index', 'labels2', 'labels5_index', 'labels5']]\n",
    "test_scaler = [*test_binary_cols, *list(map(standardizer, numeric_cols)), *['id', 'labels2_index', 'labels2', 'labels5_index', 'labels5']]\n",
    "\n",
    "scaled_train_df = (train_df.select(train_scaler).cache())\n",
    "scaled_test_df = (test_df.select(test_scaler).cache())\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=selectFeaturesByAR(ar_dict, 0.01), outputCol='raw_features')\n",
    "indexer = VectorIndexer(inputCol='raw_features', outputCol='indexed_features', maxCategories=2)\n",
    "\n",
    "prep_pipeline = Pipeline(stages=[assembler, indexer])\n",
    "prep_model = prep_pipeline.fit(scaled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "22544\n",
      "4.034746170043945\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "scaled_train_df = (prep_model\n",
    "        .transform(scaled_train_df)\n",
    "        .select('id', 'indexed_features', 'labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "scaled_test_df = (prep_model \n",
    "        .transform(scaled_test_df)\n",
    "        .select('id', 'indexed_features','labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4667979835606274383\n"
     ]
    }
   ],
   "source": [
    "# Setting seed for reproducibility\n",
    "seed = 5456476455\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100840\n",
      "25133\n"
     ]
    }
   ],
   "source": [
    "split = (scaled_train_df.randomSplit([0.8, 0.2], seed=seed))\n",
    "\n",
    "scaled_train_df = split[0].cache()\n",
    "scaled_cv_df = split[1].cache()\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_cv_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------+-------+\n",
      "| id|labels2_index|labels2|labels5|\n",
      "+---+-------------+-------+-------+\n",
      "|  9|          1.0| attack|    DoS|\n",
      "| 15|          1.0| attack|    DoS|\n",
      "| 16|          0.0| normal| normal|\n",
      "| 25|          0.0| normal| normal|\n",
      "| 26|          1.0| attack|    DoS|\n",
      "+---+-------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "25133\n",
      "22544\n"
     ]
    }
   ],
   "source": [
    "res_cv_df = scaled_cv_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "res_test_df = scaled_test_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "prob_cols = []\n",
    "pred_cols = []\n",
    "res_cv_df.show(5)\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def printCM(cm, labels):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels])\n",
    "    # Print header\n",
    "    print(\" \" * columnwidth, end=\"\\t\")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\"\\t\")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"%{0}s\".format(columnwidth) % label1, end=\"\\t\")\n",
    "        for j in range(len(labels)):\n",
    "            print(\"%{0}d\".format(columnwidth) % cm[i, j], end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "def getPrediction(e):\n",
    "    return udf(lambda row: 1.0 if row >= e else 0.0, DoubleType())\n",
    "        \n",
    "def printReport(resDF, probCol, labelCol='labels2_index', e=None, labels=['normal', 'attack']):\n",
    "    if (e):\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (1.0 if row[probCol] >= e else 0.0, row[labelCol]))\n",
    "                                       .collect()))\n",
    "    else:\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (row[probCol], row[labelCol]))\n",
    "                                       .collect()))\n",
    "    \n",
    "    cm = metrics.confusion_matrix(predictionAndLabels[1], predictionAndLabels[0])\n",
    "    printCM(cm, labels)\n",
    "    print(\" \")\n",
    "    print(\"Accuracy = %g\" % (metrics.accuracy_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\"AUC = %g\" % (metrics.roc_auc_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\" \")\n",
    "    print(\"False Alarm Rate = %g\" % (cm[0][1]/(cm[0][0] + cm[0][1])))\n",
    "    print(\"Detection Rate = %g\" % (cm[1][1]/(cm[1][1] + cm[1][0])))\n",
    "    print(\"F1 score = %g\" % (metrics.f1_score(predictionAndLabels[1], predictionAndLabels[0], labels)))\n",
    "    print(\" \")\n",
    "    print(metrics.classification_report(predictionAndLabels[1], predictionAndLabels[0]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_prob_col = 'kmeans_rf_prob'\n",
    "kmeans_pred_col = 'kmeans_rf_pred'\n",
    "\n",
    "prob_cols.append(kmeans_prob_col)\n",
    "pred_cols.append(kmeans_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\", \n",
    "                             names=list(set(selectFeaturesByAR(ar_dict, 0.1)).intersection(numeric_cols)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ar_features = nn_slicer.transform(scaled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ar_features.show(5)\n",
    "#scaled_train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nn_features.show(5)\n",
    "#pd=ar_features.select(\"features\").show(4)\n",
    "layer = [31, 10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = MultilayerPerceptronClassifier(featuresCol= \"features\", labelCol= \"labels2_index\", layers=layer, maxIter=100, seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_pipeline = Pipeline(stages=[nn_slicer,nn])\n",
    "model = nn_pipeline.fit(scaled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+-------+-------------+-------+--------------------+----------+\n",
      "| id|    indexed_features|labels2_index|labels2|labels5_index|labels5|            features|prediction|\n",
      "+---+--------------------+-------------+-------+-------------+-------+--------------------+----------+\n",
      "|  9|(77,[0,1,2,3,5,6,...|          1.0| attack|          1.0|    DoS|(31,[0,1,2,3,4,5,...|       1.0|\n",
      "| 15|(77,[0,1,2,3,5,6,...|          1.0| attack|          1.0|    DoS|(31,[0,1,2,3,4,5,...|       1.0|\n",
      "| 16|(77,[0,1,2,3,4,5,...|          0.0| normal|          0.0| normal|(31,[0,1,2,3,4,5,...|       0.0|\n",
      "| 25|(77,[0,1,2,3,4,5,...|          0.0| normal|          0.0| normal|(31,[0,1,2,3,4,5,...|       0.0|\n",
      "| 26|(77,[0,1,2,3,5,6,...|          1.0| attack|          1.0|    DoS|(31,[0,1,2,3,4,5,...|       1.0|\n",
      "+---+--------------------+-------------+-------+-------------+-------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-------+\n",
      "|            features|labels2|\n",
      "+--------------------+-------+\n",
      "|(31,[0,1,2,3,4,5,...| attack|\n",
      "|(31,[0,1,2,3,4,5,...| attack|\n",
      "|(31,[0,1,2,3,4,5,...| normal|\n",
      "|(31,[0,1,2,3,4,5,...| normal|\n",
      "|(31,[0,1,2,3,4,5,...| attack|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "result = model.transform(scaled_cv_df)\n",
    "result.show(5)\n",
    "prediction_labels = result.select(\"features\",\"labels2\").show(5)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labels2_index\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.0136872\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(result)\n",
    "print(\"Error:%g\"%(1.0-accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printReport(res_cv_df.filter(probe_exp), probCol=probe_prob_col, e=0.5, labels=['normal', 'Probe'])\n",
    "printReport(res_cv_df, probCol=probe_prob_col, e=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "      \tnormal\tattack\t\n",
    "normal\t 13238\t   211\t\n",
    "attack\t  1401\t 10342\t\n",
    " \n",
    "Accuracy = 0.974894\n",
    "AUC = 0.974247\n",
    " \n",
    "False Alarm Rate = 0.119\n",
    "Detection Rate = 0.984\n",
    "F1 score = 0.936\n",
    " \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0      0.980     0.881     0.881     13328\n",
    "        1.0      0.119     0.984     0.943     10342\n",
    "\n",
    "avg / total       0.94     0.936     0.936     23670\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
